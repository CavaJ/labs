{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and Extract MNIST dataset\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "(\" tpye of 'mnist' is \", <class 'tensorflow.contrib.learn.python.learn.datasets.mnist.DataSets'>)\n",
      " number of trian data is 55000\n",
      " number of test data is 10000\n",
      "Iteration 1000, batch cost = 16681.36719, training acc = 0.74000\n",
      "Iteration 2000, batch cost = 2922.60229, training acc = 0.72000\n",
      "Iteration 3000, batch cost = 1058.01392, training acc = 0.80000\n",
      "Iteration 4000, batch cost = 1017.96234, training acc = 0.78000\n",
      "Iteration 5000, batch cost = 392.43359, training acc = 0.86000\n",
      "Iteration 6000, batch cost = 501.31686, training acc = 0.88000\n",
      "Iteration 7000, batch cost = 375.50790, training acc = 0.87000\n",
      "Iteration 8000, batch cost = 312.42477, training acc = 0.79000\n",
      "Iteration 9000, batch cost = 450.63040, training acc = 0.81000\n",
      "Iteration 10000, batch cost = 320.63202, training acc = 0.83000\n",
      "Iteration 11000, batch cost = 444.04343, training acc = 0.84000\n",
      "Iteration 12000, batch cost = 343.02304, training acc = 0.84000\n",
      "Iteration 13000, batch cost = 186.09781, training acc = 0.90000\n",
      "Iteration 14000, batch cost = 98.36785, training acc = 0.94000\n",
      "Iteration 15000, batch cost = 164.29985, training acc = 0.93000\n",
      "Iteration 16000, batch cost = 295.81302, training acc = 0.86000\n",
      "Iteration 17000, batch cost = 216.53026, training acc = 0.85000\n",
      "Iteration 18000, batch cost = 36.57765, training acc = 0.96000\n",
      "Iteration 19000, batch cost = 118.32139, training acc = 0.94000\n",
      "Iteration 20000, batch cost = 68.11531, training acc = 0.95000\n",
      "Iteration 21000, batch cost = 209.35614, training acc = 0.89000\n",
      "Iteration 22000, batch cost = 133.17433, training acc = 0.91000\n",
      "Iteration 23000, batch cost = 74.40883, training acc = 0.94000\n",
      "Iteration 24000, batch cost = 224.92967, training acc = 0.87000\n",
      "Iteration 25000, batch cost = 174.88301, training acc = 0.89000\n",
      "Iteration 26000, batch cost = 153.03989, training acc = 0.89000\n",
      "Iteration 27000, batch cost = 117.19232, training acc = 0.93000\n",
      "Iteration 28000, batch cost = 59.24320, training acc = 0.96000\n",
      "Iteration 29000, batch cost = 18.56905, training acc = 0.96000\n",
      "Iteration 30000, batch cost = 113.07841, training acc = 0.92000\n",
      "Iteration 31000, batch cost = 125.95414, training acc = 0.90000\n",
      "Iteration 32000, batch cost = 101.99707, training acc = 0.94000\n",
      "Iteration 33000, batch cost = 27.86498, training acc = 0.98000\n",
      "Iteration 34000, batch cost = 213.25027, training acc = 0.92000\n",
      "Iteration 35000, batch cost = 139.15619, training acc = 0.93000\n",
      "Iteration 36000, batch cost = 98.15082, training acc = 0.93000\n",
      "Iteration 37000, batch cost = 84.90273, training acc = 0.92000\n",
      "Iteration 38000, batch cost = 63.08531, training acc = 0.92000\n",
      "Iteration 39000, batch cost = 71.48445, training acc = 0.93000\n",
      "Iteration 40000, batch cost = 25.19088, training acc = 0.96000\n",
      "Iteration 41000, batch cost = 141.46701, training acc = 0.86000\n",
      "Iteration 42000, batch cost = 87.31944, training acc = 0.92000\n",
      "Iteration 43000, batch cost = 131.63596, training acc = 0.87000\n",
      "Iteration 44000, batch cost = 153.66383, training acc = 0.86000\n",
      "Iteration 45000, batch cost = 100.53884, training acc = 0.88000\n",
      "Iteration 46000, batch cost = 53.57625, training acc = 0.95000\n",
      "Iteration 47000, batch cost = 189.63339, training acc = 0.85000\n",
      "Iteration 48000, batch cost = 181.09537, training acc = 0.89000\n",
      "Iteration 49000, batch cost = 62.37997, training acc = 0.95000\n",
      "Iteration 50000, batch cost = 142.96480, training acc = 0.90000\n",
      "Iteration 51000, batch cost = 42.57895, training acc = 0.96000\n",
      "Iteration 52000, batch cost = 32.65991, training acc = 0.95000\n",
      "Iteration 53000, batch cost = 50.81334, training acc = 0.92000\n",
      "Iteration 54000, batch cost = 14.49013, training acc = 0.98000\n",
      "Iteration 55000, batch cost = 38.27335, training acc = 0.98000\n",
      "Iteration 56000, batch cost = 115.50557, training acc = 0.94000\n",
      "Iteration 57000, batch cost = 19.96493, training acc = 0.95000\n",
      "Iteration 58000, batch cost = 65.01761, training acc = 0.91000\n",
      "Iteration 59000, batch cost = 212.72705, training acc = 0.89000\n",
      "Iteration 60000, batch cost = 92.27364, training acc = 0.90000\n",
      "Iteration 61000, batch cost = 55.02124, training acc = 0.95000\n",
      "Iteration 62000, batch cost = 57.40148, training acc = 0.97000\n",
      "Iteration 63000, batch cost = 77.75866, training acc = 0.94000\n",
      "Iteration 64000, batch cost = 73.64261, training acc = 0.95000\n",
      "Iteration 65000, batch cost = 69.40718, training acc = 0.94000\n",
      "Iteration 66000, batch cost = 72.34221, training acc = 0.95000\n",
      "Iteration 67000, batch cost = 57.76510, training acc = 0.90000\n",
      "Iteration 68000, batch cost = 11.72245, training acc = 0.98000\n",
      "Iteration 69000, batch cost = 0.00000, training acc = 1.00000\n",
      "Iteration 70000, batch cost = 52.57911, training acc = 0.97000\n",
      "Iteration 71000, batch cost = 18.33959, training acc = 0.97000\n",
      "Iteration 72000, batch cost = 76.87198, training acc = 0.94000\n",
      "Iteration 73000, batch cost = 35.13345, training acc = 0.95000\n",
      "Iteration 74000, batch cost = 36.09599, training acc = 0.95000\n",
      "Iteration 75000, batch cost = 84.19501, training acc = 0.93000\n",
      "Iteration 76000, batch cost = 32.95161, training acc = 0.96000\n",
      "Iteration 77000, batch cost = 33.30125, training acc = 0.97000\n",
      "Iteration 78000, batch cost = 51.81936, training acc = 0.94000\n",
      "Iteration 79000, batch cost = 48.96160, training acc = 0.97000\n",
      "Iteration 80000, batch cost = 25.93218, training acc = 0.97000\n",
      "Iteration 81000, batch cost = 71.39958, training acc = 0.93000\n",
      "Iteration 82000, batch cost = 18.74671, training acc = 0.96000\n",
      "Iteration 83000, batch cost = 39.96124, training acc = 0.92000\n",
      "Iteration 84000, batch cost = 1.10326, training acc = 0.99000\n",
      "Iteration 85000, batch cost = 81.20750, training acc = 0.92000\n",
      "Iteration 86000, batch cost = 35.74614, training acc = 0.95000\n",
      "Iteration 87000, batch cost = 61.51081, training acc = 0.94000\n",
      "Iteration 88000, batch cost = 6.92453, training acc = 0.98000\n",
      "Iteration 89000, batch cost = 18.52883, training acc = 0.98000\n",
      "Iteration 90000, batch cost = 27.28431, training acc = 0.98000\n",
      "Iteration 91000, batch cost = 19.50374, training acc = 0.97000\n",
      "Iteration 92000, batch cost = 40.96993, training acc = 0.96000\n",
      "Iteration 93000, batch cost = 28.71786, training acc = 0.96000\n",
      "Iteration 94000, batch cost = 47.91388, training acc = 0.94000\n",
      "Iteration 95000, batch cost = 42.80415, training acc = 0.96000\n",
      "Iteration 96000, batch cost = 87.41737, training acc = 0.93000\n",
      "Iteration 97000, batch cost = 47.44620, training acc = 0.94000\n",
      "Iteration 98000, batch cost = 80.98949, training acc = 0.89000\n",
      "Iteration 99000, batch cost = 65.80569, training acc = 0.94000\n",
      "\n",
      "Results:\n",
      "\n",
      "Test Accuracy: 0.9402\n"
     ]
    }
   ],
   "source": [
    "#PART 2 OF ASSIGMENT 4, Build and train deeper CNNs with tf.nn.dropout\n",
    "# • Add more Conv layers\n",
    "# • Add dropout function after each pooling layer\n",
    "# best performance achieved is 0.98\n",
    "# only visualization part is not done\n",
    "\n",
    "print (\"Download and Extract MNIST dataset\")\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True)\n",
    "print (\" tpye of 'mnist' is \", type(mnist))\n",
    "print (\" number of trian data is %d\" % (mnist.train.num_examples))\n",
    "print (\" number of test data is %d\" % (mnist.test.num_examples))\n",
    "\n",
    "# learning arameters\n",
    "learning_rate = 0.01\n",
    "noOfIterations = 100000\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "# net parameters\n",
    "n_input_width = 28 # MNIST images are 28 pixels * 28 pixels = 784\n",
    "n_input_height = 28\n",
    "\n",
    "n_output = 10 # classes (0-9 digits)\n",
    "dropout = 0.9 # dropout, prob. to keep units in the process\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input_width * n_input_height])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout in feed_dict\n",
    "\n",
    "# helper function to do conv2d, bias adding and relu activation together\n",
    "def conv2d(input, weights, biases):\n",
    "    # Convolution\n",
    "    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # Add-bias\n",
    "    bias_addition = tf.nn.bias_add(conv, biases)\n",
    "    # Pass ReLu\n",
    "    relu_result = tf.nn.relu(bias_addition)\n",
    "    return relu_result\n",
    "\n",
    "# helper function to do \"k\" max-pooling\n",
    "def max_pool(input, k):\n",
    "    return tf.nn.max_pool(input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# helper function to apply dropout to an intended layer\n",
    "def apply_dropout(layer, dropout):\n",
    "    return tf.nn.dropout(layer, dropout)\n",
    "\n",
    "# helper fucntion to costruct network model\n",
    "def construct_model(X, weights, biases, dropout):\n",
    "    \n",
    "    # reshape input image\n",
    "    input_r = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # convolution layer 1\n",
    "    conv1 = conv2d(input_r, weights['wc1'], biases['bc1'])\n",
    "    # max pooling with k = 2\n",
    "    pool1 = max_pool(conv1, k=2)\n",
    "    # apply dropout after pooling layer\n",
    "    pool1_drop = apply_dropout(pool1, dropout)\n",
    "\n",
    "    # convolution layer 2\n",
    "    conv2 = conv2d(pool1_drop, weights['wc2'], biases['bc2'])\n",
    "    # max pooling with k = 2\n",
    "    pool2 = max_pool(conv2, k=2)\n",
    "    # apply dropout after pooling layer\n",
    "    pool2_drop = apply_dropout(pool2, dropout)\n",
    "\n",
    "    # fully connected layer\n",
    "    dense = tf.reshape(pool2_drop, [-1, weights['wf1'].get_shape().as_list()[0]]) # vectorize pool2_drop\n",
    "    relu = tf.nn.relu(tf.add(tf.matmul(dense, weights['wf1']), biases['bf1'])) # apply relu\n",
    "    relu_drop = apply_dropout(relu, dropout) # apply dropout\n",
    "\n",
    "    # output - class prediction\n",
    "    out = tf.add(tf.matmul(relu_drop, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# define weights and biases arrays\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 convolution, 1 input channel, 32 filters\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), # 5x5 convolution, 32 input channel, 64 filters\n",
    "    'wf1': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])), # fully connected, 7 * 7 * 64 inputs, 1024 outputs\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_output])) # 1024 inputs, 10 class outputs\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bf1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}\n",
    "\n",
    "# construct model\n",
    "pred = construct_model(x, weights, biases, keep_prob)\n",
    "\n",
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# test model for evaluation\n",
    "predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "\n",
    "# init the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# open session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # define number of steps to print training accuracy, and cost of the batch\n",
    "    iter = 1\n",
    "    \n",
    "    # training, stop when maximum noOfIterations reached\n",
    "    while iter * batch_size < noOfIterations:\n",
    "        \n",
    "        # get bacth instances\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "        # fit training using batch\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "            \n",
    "        if iter % display_step == 0:\n",
    "            \n",
    "            # calculate accuracy in the batch\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            \n",
    "            # calculate cost function in the batch\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            \n",
    "            print \"Iteration \" + str(iter * batch_size) + \", batch cost = \" + \"{:.5f}\".format(loss) + \", training acc = \" + \"{:.5f}\".format(acc)\n",
    "        \n",
    "        iter += 1\n",
    "\n",
    "    print \"\\nResults:\\n\"\n",
    "\n",
    "    # calculate test accuracy\n",
    "    print \"Test Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.})\n",
    "    \n",
    "    # ONLY VISUALIZATION PART IS NOT DONE\n",
    "    #for i in range(64):\n",
    "        #plt.matshow(pool[0, :, :, i], cmap=plt.get_cmap('gray'))\n",
    "        #plt.title(str(i) + \"th pool\")\n",
    "        #plt.colorbar()\n",
    "        #plt.show()\n",
    "    \n",
    "    writer = tf.train.SummaryWriter('/tmp/tf_logs/cnn_model',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
